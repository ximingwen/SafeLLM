{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3af40bb6-16ab-4f61-b967-ae4779c72036",
   "metadata": {},
   "source": [
    "# CLIP Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3915bde1-e2c2-40ef-bdb2-6a5c88735d4e",
   "metadata": {},
   "source": [
    "Author: xiaodongguaAIGC\n",
    "\n",
    "our target is to make Pytorch-implemention about CLIP, we could ref follow code, we simplfied image_encoder & text_encoder, take more attention about CLIP loss\n",
    "\n",
    "CLIP paper : [Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/pdf/2103.00020)\n",
    "\n",
    "![clip](./images/clip.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae866a9a-617b-457d-a4f1-862114028b69",
   "metadata": {},
   "source": [
    "peseudocode Numpy-Like Clip loss implemention\n",
    "\n",
    "```python\n",
    "# image_encoder - ResNet or Vision Transformer\r\n",
    "# text_encoder - CBOW or Text Transformer\r\n",
    "# I[n, h, w, c] - minibatch of aligned images\r\n",
    "# T[n, l] - minibatch of aligned texts\r\n",
    "# W_i[d_i, d_e] - learned proj of image to embed\r\n",
    "# W_t[d_t, d_e] - learned proj of text to embed\r\n",
    "# t - learned temperature parameter\r\n",
    "# extract feature representations of each modality\r\n",
    "I_f = image_encoder(I) #[n, d_i]\r\n",
    "T_f = text_encoder(T) #[n, d_t]\r\n",
    "# joint multimodal embedding [n, d_e]\r\n",
    "I_e = l2_normalize(np.dot(I_f, W_i), axis=1)\r\n",
    "T_e = l2_normalize(np.dot(T_f, W_t), axis=1)\r\n",
    "# scaled pairwise cosine similarities [n, n]\r\n",
    "logits = np.dot(I_e, T_e.T) * np.exp(t)\r\n",
    "# symmetric loss function\r\n",
    "labels = np.arange(n)\r\n",
    "loss_i = cross_entropy_loss(logits, labels, axis=0)\r\n",
    "loss_t = cross_entropyls, axis=1)\n",
    "loss   = (loss_i + loss_t)/2\n",
    "\n",
    "```loss_i + loss_t)/2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7480bd9e-463d-45b6-8b86-cf8b938ab0f8",
   "metadata": {},
   "source": [
    "# config "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d7b69a9c-3a96-4e88-a602-5901c3a340ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# image\n",
    "batch_size = 8\n",
    "height = 2\n",
    "width = 2\n",
    "chanel = 3\n",
    "d_i = 4\n",
    "d_e = 5\n",
    "\n",
    "# text\n",
    "seq_len = 2 \n",
    "d_t = 3\n",
    "vocab_size = 100\n",
    "dim = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0370ed18-2fb4-4f5b-bf8c-d3f33d03b47f",
   "metadata": {},
   "source": [
    "# Image encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5188a342-6262-4af9-8a97-335ec057d5b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">torch.Size</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span><span style=\"font-weight: bold\">])</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m8\u001b[0m, \u001b[1;36m12\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ImageEncoder</span><span style=\"font-weight: bold\">(</span>\n",
       "  <span style=\"font-weight: bold\">(</span>encoder<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Linear</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">in_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span>, <span style=\"color: #808000; text-decoration-color: #808000\">out_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>, <span style=\"color: #808000; text-decoration-color: #808000\">bias</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mImageEncoder\u001b[0m\u001b[1m(\u001b[0m\n",
       "  \u001b[1m(\u001b[0mencoder\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m12\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m4\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">torch.Size</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">])</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m8\u001b[0m, \u001b[1;36m3\u001b[0m, \u001b[1;36m2\u001b[0m, \u001b[1;36m2\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">torch.Size</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"font-weight: bold\">])</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m8\u001b[0m, \u001b[1;36m4\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class ImageEncoder(nn.Module):\n",
    "    def __init__(self, height, width, chanel, d_i):\n",
    "        super().__init__()\n",
    "        self.input_dim = height*width*chanel\n",
    "        self.output_dim = d_i\n",
    "        self.encoder = nn.Linear(self.input_dim, self.output_dim, bias=False) \n",
    "    def forward(self, x):\n",
    "        x_flat = x.flatten(1) # batchsize, c, h, w -> batchsize, c*h*w\n",
    "        y = self.encoder(x_flat)\n",
    "        return y\n",
    "\n",
    "I = torch.randn(batch_size, chanel, height, width)\n",
    "print(I.flatten(1).shape)\n",
    "\n",
    "image_encoder = ImageEncoder(height, width, chanel, d_i)\n",
    "print(image_encoder)\n",
    "I_f = image_encoder(I)\n",
    "print(I.shape)\n",
    "print(I_f.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80a5ad2-39cc-4e86-9beb-9d133b98e9a0",
   "metadata": {},
   "source": [
    "# Text encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "39e6fc42-6313-430f-9dc8-cf597f235c67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">TextEncoder</span><span style=\"font-weight: bold\">(</span>\n",
       "  <span style=\"font-weight: bold\">(</span>embedding<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Embedding</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">100</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">512</span><span style=\"font-weight: bold\">)</span>\n",
       "  <span style=\"font-weight: bold\">(</span>encoder<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Linear</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">in_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">512</span>, <span style=\"color: #808000; text-decoration-color: #808000\">out_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">512</span>, <span style=\"color: #808000; text-decoration-color: #808000\">bias</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">)</span>\n",
       "  <span style=\"font-weight: bold\">(</span>output<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Linear</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">in_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">512</span>, <span style=\"color: #808000; text-decoration-color: #808000\">out_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>, <span style=\"color: #808000; text-decoration-color: #808000\">bias</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mTextEncoder\u001b[0m\u001b[1m(\u001b[0m\n",
       "  \u001b[1m(\u001b[0membedding\u001b[1m)\u001b[0m: \u001b[1;35mEmbedding\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m100\u001b[0m, \u001b[1;36m512\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mencoder\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m512\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m512\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0moutput\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m512\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m3\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">torch.Size</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">])</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m8\u001b[0m, \u001b[1;36m2\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">torch.Size</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"font-weight: bold\">])</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m8\u001b[0m, \u001b[1;36m3\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Text encoder is CASUAL-LANGUAGE-Modeling， \n",
    "# Attention mask is tril be like\n",
    "# 1 0 0\n",
    "# 1 1 0\n",
    "# 1 1 1\n",
    "# and we have 3 token “hello world <EOS>” \n",
    "# <EOS> output logits as text encoder feature\n",
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, dim, d_t):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.d_t = d_t\n",
    "        self.embedding = nn.Embedding(vocab_size, self.dim)\n",
    "        self.encoder = nn.Linear(self.dim, self.dim, bias=False)\n",
    "        self.output = nn.Linear(self.dim, self.d_t, bias=False) \n",
    "    def forward(self, x):\n",
    "        # x_flat = x.flatten(1) # batchsize, c, h, w -> batchsize, c*h*w\n",
    "        x_embd = self.embedding(x)\n",
    "        y = self.encoder(x_embd)\n",
    "        # 文本里取 \"[cls] token1 token2 token3\" -> token3 对应的特征向量\n",
    "        out = self.output(y)[:,-1,:] # <EOS> output logits as text encoder feature\n",
    "        return out\n",
    "\n",
    "I = torch.randn(batch_size, chanel, height, width)\n",
    "# print(I.flatten(1).shape)\n",
    "T = torch.randint(low=0, high=vocab_size, \n",
    "                  size=(batch_size, seq_len),\n",
    "                  dtype=torch.int)\n",
    "\n",
    "text_encoder = TextEncoder(vocab_size, dim, d_t)\n",
    "print(text_encoder)\n",
    "T_f = text_encoder(T) # if cls token \n",
    "print(T.shape)\n",
    "print(T_f.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55499e0-debb-454e-96d3-5f9a6d59fd5a",
   "metadata": {},
   "source": [
    "# Clip loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ffdad644-45ca-4c3e-9d42-2fac7b1fd4fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">torch.Size</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"font-weight: bold\">])</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m8\u001b[0m, \u001b[1;36m4\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">torch.Size</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"font-weight: bold\">])</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m8\u001b[0m, \u001b[1;36m3\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">CLIP</span><span style=\"font-weight: bold\">(</span>\n",
       "  <span style=\"font-weight: bold\">(</span>W_i_e<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Linear</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">in_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>, <span style=\"color: #808000; text-decoration-color: #808000\">out_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>, <span style=\"color: #808000; text-decoration-color: #808000\">bias</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">)</span>\n",
       "  <span style=\"font-weight: bold\">(</span>W_t_e<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Linear</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">in_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>, <span style=\"color: #808000; text-decoration-color: #808000\">out_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>, <span style=\"color: #808000; text-decoration-color: #808000\">bias</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">)</span>\n",
       "  <span style=\"font-weight: bold\">(</span>loss_fn<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">CrossEntropyLoss</span><span style=\"font-weight: bold\">()</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mCLIP\u001b[0m\u001b[1m(\u001b[0m\n",
       "  \u001b[1m(\u001b[0mW_i_e\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m4\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m5\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mW_t_e\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m3\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m5\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mloss_fn\u001b[1m)\u001b[0m: \u001b[1;35mCrossEntropyLoss\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">output feature loss:  <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5.7924</span>, <span style=\"color: #808000; text-decoration-color: #808000\">grad_fn</span>=<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">AddBackward0</span><span style=\"font-weight: bold\">&gt;)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "output feature loss:  \u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m5.7924\u001b[0m, \u001b[33mgrad_fn\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mAddBackward0\u001b[0m\u001b[1m>\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">output feature loss_i:  <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2.9474</span>, <span style=\"color: #808000; text-decoration-color: #808000\">grad_fn</span>=<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">NllLossBackward0</span><span style=\"font-weight: bold\">&gt;)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "output feature loss_i:  \u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m2.9474\u001b[0m, \u001b[33mgrad_fn\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mNllLossBackward0\u001b[0m\u001b[1m>\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">output feature loss_t:  <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2.8450</span>, <span style=\"color: #808000; text-decoration-color: #808000\">grad_fn</span>=<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">NllLossBackward0</span><span style=\"font-weight: bold\">&gt;)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "output feature loss_t:  \u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m2.8450\u001b[0m, \u001b[33mgrad_fn\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mNllLossBackward0\u001b[0m\u001b[1m>\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">output feature image_embedding: \n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">torch.Size</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span><span style=\"font-weight: bold\">])</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "output feature image_embedding: \n",
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m8\u001b[0m, \u001b[1;36m5\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">output feature text_embedding: \n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">torch.Size</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span><span style=\"font-weight: bold\">])</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "output feature text_embedding: \n",
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m8\u001b[0m, \u001b[1;36m5\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">output feature logits: \n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">torch.Size</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span><span style=\"font-weight: bold\">])</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "output feature logits: \n",
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m8\u001b[0m, \u001b[1;36m8\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "class CLIP(nn.Module):\n",
    "    def __init__(self, d_i, d_t, d_e):\n",
    "        super().__init__()\n",
    "        self.W_i_e = nn.Linear(d_i, d_e, bias=False)\n",
    "        self.W_t_e = nn.Linear(d_t, d_e, bias=False)\n",
    "        self.temparture = nn.Parameter(torch.ones(1))\n",
    "        # self.softmax_i = nn.Softmax(dim=0)\n",
    "        # self.softmax_t = nn.Softmax(dim=1)\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "    \n",
    "    def forward(self, I_f, T_f, loss_type='basic'):\n",
    "        n, _ = I_f.size()\n",
    "        \n",
    "        I_e = self.W_i_e(I_f) # image_embedding\n",
    "        T_e = self.W_t_e(T_f) # text_embedding\n",
    "\n",
    "        # I_e = F.normalize(I_e, p=2, dim=1)\n",
    "        # T_e = F.normalize(T_e, p=2, dim=1)\n",
    "        I_e = I_e / I_e.norm(p=2, dim=-1, keepdim=True)\n",
    "        T_e = T_e / T_e.norm(p=2, dim=-1, keepdim=True)\n",
    "\n",
    "        logits = I_e @ T_e.transpose(1,0)\n",
    "        logits = logits * torch.exp(self.temparture)\n",
    "\n",
    "        labels = torch.arange(n)\n",
    "\n",
    "        loss_i = self.loss_fn(logits, labels)\n",
    "        loss_t = self.loss_fn(logits.transpose(1,0), labels)\n",
    "        \n",
    "        loss = loss_i + loss_t\n",
    "        \n",
    "        return {\n",
    "            'image_embedding': I_e,\n",
    "            'text_embedding': T_e,\n",
    "            'logits': logits,\n",
    "            'loss' : loss,\n",
    "            'loss_i' : loss_i,\n",
    "            'loss_t' : loss_t,\n",
    "        }\n",
    "\n",
    "print(I_f.shape)\n",
    "print(T_f.shape)\n",
    "\n",
    "clip = CLIP(d_i, d_t, d_e)\n",
    "print(clip)\n",
    "output = clip(I_f, T_f)\n",
    "\n",
    "print('output feature loss: ', output['loss'])\n",
    "print('output feature loss_i: ', output['loss_i'])\n",
    "print('output feature loss_t: ', output['loss_t'])\n",
    "\n",
    "print('output feature image_embedding: ', output['image_embedding'].shape)\n",
    "print('output feature text_embedding: ', output['text_embedding'].shape)\n",
    "print('output feature logits: ', output['logits'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2eac2f-efb4-4838-bfbe-2a9e21212582",
   "metadata": {},
   "source": [
    "# CLIP Loss Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "da6bfb78-24a3-4b63-9c2b-efdab97e1ca8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(8.1138, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# step 1: create image & text data\n",
    "I = torch.randn(batch_size, chanel, height, width)\n",
    "T = torch.randint(low=0, high=vocab_size, \n",
    "                  size=(batch_size, seq_len),\n",
    "                  dtype=torch.int)\n",
    "\n",
    "# step 2: create image encoder, text encoder, clip modeling\n",
    "image_encoder = ImageEncoder(height, width, chanel, d_i)\n",
    "text_encoder = TextEncoder(vocab_size, dim, d_t)\n",
    "clip = CLIP(d_i, d_t, d_e)\n",
    "\n",
    "# step 3: compute loss\n",
    "I_f = image_encoder(I) # this is image represention\n",
    "T_f = text_encoder(T)  # this is text represention\n",
    "output = clip(I_f, T_f)\n",
    "\n",
    "# step 4: update clip-> parameters of \"image encoder or text encoder\" \n",
    "output['loss'] \n",
    "# output['loss'].backward()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
